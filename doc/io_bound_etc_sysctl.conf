# 高网络IO型机器内核参数配置及说明
# tuned service 里面也有相关配置 可能会影响这里配置

# net.core.rmem/wmem
# 内核网络收发缓冲区配置，同时决定tcp和udp得最大值 单位字节
# udp环境下必须设置该值 不然容易导致大流量下丢包
# udp同时还需要代码中调用 SO_RECVBUF
# 默认值208K
# 16777216=16M
net.core.rmem_max=16777216
net.core.rmem_default=16777216
net.core.wmem_max=16777216
net.core.wmem_default=16777216

# 页为单位的 缓存设置 一般不设置系统会根据内存大小自动调节
# net.ipv4.udp_mem = 262144 1048576 4194304
# net.ipv4.tcp_mem = 262144 1048576 4194304

# optmem_max
# net.core.optmem_max 限制了单个套接字可使用的额外选项内存上限（单位：字节）。
# 当网络应用需要启用高级特性（如大窗口、加密握手）时，内核会动态分配额外内存。
# 若此值过小，可能导致：高级 TCP 特性无法启用（如窗口缩放因子被限制),握手阶段频繁失败（如 TLS 握手需要更多内存）。
# 在高带宽（10Gbps+）或高并发（万级连接）场景下，建议设置为 256KB 到 512KB：
# 系统默认值20480
net.core.optmem_max=524288
# netdev_max_backlog
# 用于控制网络设备接收队列（RX Queue）的最大长度。
# 当网络数据包到达速率超过内核处理能力时，未处理的数据包会暂时存储在这个队列中。
# 合理设置该参数可以避免因队列溢出导致的数据包丢失，提升网络性能
# 默认值一般是3000,1G左右流量建议值3000-8192
# 高并发服务器（10Gbps+）	16384-大到内存能接受的程度以应对高速网络的流量突发
# 低延迟数据库或者游戏服务器 建议4096
# 不过该值收网卡驱动实现有关，和ethtool和rx-queus关系不明以及ip link show quelen 
# 2k*60000 = 120,000 120M内存可控
net.core.netdev_max_backlog=60000
# netdev_budget
# 用于控制每次网络中断处理周期中允许处理的最大数据包数量。
# 这个参数平衡了网络处理效率与系统响应性：
# 较大的值可以提高网络吞吐量，但可能降低系统对其他任务的响应能力；较小的值则反之
# 大流量场景建议调大该值，低延迟要求高如游戏服务器建议调低该值
# 但是太大容易导致si过高
# 默认值一般300
net.core.netdev_budget=600
### RPS/RFS/RSS相关
###普通的物理网卡不建议配置，否则可能会造成核间中断开销增大，反而降低性能。
###RFS依赖于RPS，RPS是默认关闭的，需要开启RPS后，RFS相关配置才能生效。RPS配置CPU列表时，尽量选取与中断CPU同一个NUMA域内的CPU，同时也应尽量排除负载已经很高的CPU。
###云服务器使用虚拟网卡时（例如veth），通过配置rps_sock_flow_entries参数值，可较大程度提升网络性能。
### echo ff > /sys/class/net/eth0/queues/rx-*/rps_cpus
### echo 4096 > /sys/class/net/eth0/queues/rx-*/rps_flow_cnt
#### net.core.rps_sock_flow_entries=32768

#!/bin/bash
# Enable RPS (Receive Packet Steering)

# rfc=4096
# cc=$(grep -c processor /proc/cpuinfo)
# rsfe=$(echo $cc*$rfc | bc)
# sysctl -w net.core.rps_sock_flow_entries=$rsfe
# for fileRps in $(ls /sys/class/net/eth*/queues/rx-*/rps_cpus)
# do
#     echo fff > $fileRps
# done

# for fileRfc in $(ls /sys/class/net/eth*/queues/rx-*/rps_flow_cnt)
# do
#     echo $rfc > $fileRfc
# done

# tail /sys/class/net/eth*/queues/rx-*/{rps_cpus,rps_flow_cnt}

# qdisc
# 网络设备或多或少都有 buffer
# 初衷是以增大少许延迟来避免丢包
# 然而 buffer 的存在所导致的延迟可能干扰 TCP 对链接质量的判断
# buffer 最终被塞满，丢包不可避免，反而新引入了延迟波动问题，这一现象被称为 bufferbloat，
# 老些的内核默认是pfifo_fast 
# (减少 CPU 开销，但它默认队列过长很容易引起 bufferbloat，不能识别网络流可能导致部分流被饿死)
# 新的内核默认是fq_codel 
#（ 使用 BQL 动态控制 buffer 大小，自动区分“好流”和“坏流”，
# fq_codel 对非常小的流进行了优化避免饿死问题）
# 服务端开发流量大建议使用fq,启用BBR也依赖fq
# 因无法启用BBR暂时不改这个
# 同时要和ip link show eth0 的qdisc区分开
# 这个是用tc （trafic control）控制的
# 一般的网络增强型的 ip link show eth0 qdisc(mq)
#      普通虚机的是 pfifo_fast
# net.core.default_qdisc=fq

# 拥塞控制算法 默认使用的cubic,可以改用BBR要求内核4.9版本以上
#### net.ipv4.tcp_congestion_control=bbr

# tcp tcp_moderate_rcvbuf
# tcp基于动态调整，一般不设置SO_RECVBUF 设置后了跟客户端设置有关，还可能增大延迟
# 一般使用系统默认的动态调整 但是动态调整必须开启tcp_moderate_rcvbuf 
# 这里设置1表示是不是已经开启都开启
net.ipv4.tcp_moderate_rcvbuf=1
# tcp rmem/wmem 有设置3个参数表示最小值 默认值 和最大值 最大不能超过core rmem_max
# 推荐参数16K 1M 16M
net.ipv4.tcp_rmem=16384  1048576  16777216  
net.ipv4.tcp_wmem=16384  1048576  16777216

# 扩充tcp滑动窗口参数 默认开启,这里设置好 防止没有开启的时候能够开启
net.ipv4.tcp_window_scaling=1
# SACK(Selective Acknowledgment)使tcp交互双发只重新发送交互过程中丢失的包
# 需要建立链接的时候双方都带该选项
# 服务端一般默认开启，这里设置好，防止没有开启的时候是开启
net.ipv4.tcp_sack=1
# DACK用于控制TCP重复SACKDuplicate SACK功能的启用状态。DSACK是TCP协议的扩展特性（RFC2883）旨在更高效地处理丢包和乱序问题，提升网络传输可靠性和性能。
# 服务端一般默认开启，这里设置好，防止没有开启的时候是开启
# 卫星网络通信质量很差的情况下建议关闭
net.ipv4.tcp_dack=1
# FACK用于控制TCP Forward SACK（F-SACK）功能的启用状态。F-SACK 是 TCP SACK（选择性确认）机制的扩展，旨在更精确地报告乱序和丢包情况，帮助发送方优化重传策略。
# 有助于发送方区分丢包和乱序，避免不必要的重传
# 服务端一般默认开启，这里设置好，防止没有开启的时候是开启
net.ipv4.tcp_fack=1

# 收包合并 抓包看到几万字节或者几千字节都是1500的小包合并了大包 要确认开启的
#### ethtool -k eth0 | grep generic-receive-offload 
#### 开启使用 ethtool -K eth0 gro on

# media server relevant kernel net module settings
net.core.rmem_max = 16777216
net.core.rmem_default = 16777216
net.core.wmem_max = 16777216
net.core.wmem_default = 16777216
net.core.optmem_max = 524288
net.core.netdev_max_backlog = 60000
net.core.netdev_budget = 600
net.ipv4.tcp_window_scaling = 1
net.ipv4.tcp_sack = 1
net.ipv4.tcp_dsack = 1
net.ipv4.tcp_fack = 1
net.ipv4.tcp_moderate_rcvbuf = 1
net.ipv4.tcp_rmem = 16384  1048576  16777216
net.ipv4.tcp_wmem = 16384  1048576  16777216
# media server relevant kernel net module settings